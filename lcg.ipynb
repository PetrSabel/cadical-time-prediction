{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0' \n",
    "config = {\n",
    "    \"embedding\": 32, # Node embeddings size \n",
    "    \"batch\": 512, # Batch size \n",
    "    \"epochs\": 1000, # Training \n",
    "    \"target\": \"seconds\", # sat OR seconds (classification vs regression)\n",
    "    \"log\": False, # Whether model should be trained in log-space \n",
    "    \"dataset\": \"cnf.csv\", # File with dataset\n",
    "    \"layers\": 3, # Number of GNN layers \n",
    "    \"transformation\": \"relu\", # Node transformation before applying GNN (relu, linear, mlp)\n",
    "    \"gnn\": \"MLPGraphNorm\", # Actual graph network architecture (SAGEConv, SAGE_MLP, GINConv, MLP_GIN, GATConv, MLPGraphNorm)\n",
    "    \"projection\": \"mlp\", # How to combine neighbor embeddings (linear, mlp, mlp-3)\n",
    "    \"readout\": \"multi\", # How to aggregate final node embeddings (lstm, mean, sum, multi)\n",
    "    \"dropout\": False,\n",
    "    \"jumping\": True, # Whether apply jumping knowledge technique (sum over past values) \n",
    "    \"encoding\": \"lcg\",\n",
    "    \"small\": False, # Use subset of training data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysat.formula import CNF\n",
    "from itertools import permutations \n",
    "import torch \n",
    "import pandas as pd \n",
    "from copy import deepcopy \n",
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy \n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.nn import Sequential, ReLU, Linear \n",
    "import torch.nn.functional as F \n",
    "\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv, GAT, GATConv, GINConv \n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.aggr import MLPAggregation, LSTMAggregation, SumAggregation, MultiAggregation \n",
    "from torch_geometric.nn.models import MLP \n",
    "from torch_geometric.utils import degree\n",
    "from torch import Tensor\n",
    "\n",
    "import torchmetrics\n",
    "from torch_geometric.nn import summary \n",
    "import time \n",
    "import os \n",
    "import random \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CNF file and transform to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_literal_embedding = torch.ones(config['embedding'])\n",
    "initial_clause_embedding = torch.ones(config['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_literal(l, n_vars):\n",
    "    \"Return index of the literal.\"\n",
    "    # First go positives, then negatives. Ex. [1, 2, 3, -1, -2, -3] \n",
    "    return l-1 if l > 0 else (-l-1)+n_vars \n",
    "\n",
    "def lig_graph(filepath):\n",
    "    # LIG encoding \n",
    "    cnf = CNF(from_file=filepath)\n",
    "    n_vars = cnf.nv \n",
    "    edges = [] \n",
    "    for clause in cnf.clauses:\n",
    "        for (x_i, x_j) in permutations(clause, r=2):\n",
    "            x_i = index_literal(x_i, n_vars)\n",
    "            x_j = index_literal(x_j, n_vars)\n",
    "            edges.append((x_i, x_j))\n",
    "    edges = list(set(edges)) # Remove duplicates \n",
    "    # Add links between literals of same variable\n",
    "    edges.extend([(i, i+n_vars) for i in range(n_vars)])\n",
    "    edges.extend([(i+n_vars, i) for i in range(n_vars)]) # Both directions \n",
    "    edges = torch.Tensor(edges).to(torch.int64).T \n",
    "\n",
    "    # 2 literals for each variable \n",
    "    x = initial_literal_embedding.repeat(n_vars*2, 1) # Initialize node embeddings to ones \n",
    "    \n",
    "    return x, edges \n",
    "\n",
    "def lcg_graph(filepath):\n",
    "    # LCG encoding \n",
    "    cnf = CNF(from_file=filepath)\n",
    "    n_vars = cnf.nv \n",
    "    lc_edges = [] \n",
    "    cl_edges = [] \n",
    "    for i, clause in enumerate(cnf.clauses):\n",
    "        for x_i in clause:\n",
    "            x_i = index_literal(x_i, n_vars) \n",
    "            # Note: literals and clause has distinct indices \n",
    "            lc_edges.append((x_i, i)) \n",
    "            cl_edges.append((i, x_i)) \n",
    "\n",
    "    # Add links between literals of same variable\n",
    "    literal_edges = [(i, i+n_vars) for i in range(n_vars)] \n",
    "    literal_edges.extend([(i+n_vars, i) for i in range(n_vars)]) # Both directions \n",
    "\n",
    "    lc_edges = torch.Tensor(lc_edges).to(torch.int64).T \n",
    "    cl_edges = torch.Tensor(cl_edges).to(torch.int64).T \n",
    "    literal_edges = torch.Tensor(literal_edges).to(torch.int64).T \n",
    "\n",
    "    # 2 literals for each variable \n",
    "    x_vars = initial_literal_embedding.repeat(n_vars*2, 1) # copy embedding for each literal \n",
    "\n",
    "    x_clauses = initial_clause_embedding.repeat(len(cnf.clauses), 1) # copy embedding for each literal \n",
    "    \n",
    "    return (x_vars, x_clauses), (literal_edges, lc_edges, cl_edges) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process cnf files into graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formulas = pd.read_csv(config['dataset'])[['filename', 'seconds', 'sat']] \n",
    "\n",
    "formulas = formulas.dropna() # Take only valid data \n",
    "formulas = formulas.sample(frac=1, random_state=12345).reset_index(drop=True) # Random permutation \n",
    "\n",
    "if config['small']:\n",
    " formulas = formulas[:2000] # Choose a subset \n",
    "print(f\"SAT are {sum(formulas['sat'] == True)}, UNSAT are {sum(formulas['sat'] == False)}\")\n",
    "\n",
    "\n",
    "if config['log']:\n",
    "    formulas['seconds'] = numpy.log(formulas['seconds'])  # log10\n",
    "formulas['seconds'].hist() # bins=[0, 10, 100, 500]\n",
    "\n",
    "\n",
    "mapping = {} \n",
    "dataset = [] \n",
    "batches = [] \n",
    "current_batch = [] \n",
    "for i, (file, seconds, sat) in enumerate(tqdm(formulas.values)):  \n",
    "    x, edges = lcg_graph(file) \n",
    "    data = HeteroData() \n",
    "    # Add node features \n",
    "    data['literals'].x = x[0] \n",
    "    data['clauses'].x = x[1] \n",
    "    # Add edges \n",
    "    data['literals', 'negates', 'literals'].edge_index = edges[0] \n",
    "    data['literals', 'inside', 'clauses'].edge_index = edges[1] \n",
    "    data['clauses', 'contains', 'literals'].edge_index = edges[2] \n",
    "    # Add graph label \n",
    "    if config['target'] == \"sat\":\n",
    "        data.y = float(sat) \n",
    "    else:\n",
    "        data.y = seconds \n",
    "    data.validate() # Throw error if graph is not valid \n",
    "    dataset.append(data) \n",
    "    mapping[i] = file \n",
    "print(f\"Total dataset has {len(dataset)} graphs\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPGraphConv(MessagePassing):\n",
    "    def __init__(self, mlp, aggr='mean'):\n",
    "        super().__init__(aggr=aggr)  # Aggregation can be 'mean', 'add', or 'max'\n",
    "        # Define the MLP\n",
    "        self.mlp = mlp \n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Perform message passing\n",
    "        return self.propagate(edge_index, x=x)\n",
    "\n",
    "    def message(self, x_j):\n",
    "        # Apply the MLP to the source node features\n",
    "        return self.mlp(x_j)\n",
    "\n",
    "\n",
    "class SAGE_MLP(SAGEConv):\n",
    "    # Only apply MLP on node embedddings before message propagation \n",
    "    def __init__(self, mlp, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs) \n",
    "        # Define the MLP\n",
    "        self.mlp = deepcopy(mlp) \n",
    "\n",
    "    def message(self, x_j):\n",
    "        # Apply the MLP to the source node features\n",
    "        return self.mlp(x_j)\n",
    "\n",
    "\n",
    "# With normalization \n",
    "class MLPGraphNorm(MessagePassing):\n",
    "    def __init__(self, mlp, aggr='mean'):\n",
    "        super(MLPGraphNorm, self).__init__(aggr=aggr)  # Aggregation can be 'mean', 'add', or 'max'\n",
    "        # Define the MLP\n",
    "        self.mlp = mlp\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node feature matrix or a tuple (x_src, x_dst) for bipartite graphs.\n",
    "            edge_index: Edge index tensor.\n",
    "        \"\"\"\n",
    "        if isinstance(x, tuple):\n",
    "            x_src, x_dst = x\n",
    "        elif isinstance(x, torch.Tensor):\n",
    "            x_src = x_dst = x\n",
    "        else:\n",
    "            print(\"HERE\", \"STRANDE\")\n",
    "            print(type(x)) \n",
    "            print(x, x.shape)\n",
    "            print(stange)\n",
    "\n",
    "        # Number of source and target nodes\n",
    "        num_src, num_dst = x_src.size(0), x_dst.size(0)\n",
    "        \n",
    "        # Compute degree for source nodes (row normalization)\n",
    "        row, col = edge_index\n",
    "        deg_src = degree(row, num_src, dtype=x_src.dtype)\n",
    "        deg_inv_sqrt_src = deg_src.pow(-0.5)\n",
    "        deg_inv_sqrt_src[deg_inv_sqrt_src == float('inf')] = 0\n",
    "\n",
    "        # Compute degree for target nodes (col normalization)\n",
    "        deg_dst = degree(col, num_dst, dtype=x_dst.dtype)\n",
    "        deg_inv_sqrt_dst = deg_dst.pow(-0.5)\n",
    "        deg_inv_sqrt_dst[deg_inv_sqrt_dst == float('inf')] = 0\n",
    "\n",
    "        # Perform message passing\n",
    "        return self.propagate(edge_index, x=(x_src, x_dst), \n",
    "                              deg_inv_sqrt_src=deg_inv_sqrt_src, \n",
    "                              deg_inv_sqrt_dst=deg_inv_sqrt_dst)\n",
    "\n",
    "    def message(self, x_j, edge_index, deg_inv_sqrt_src, deg_inv_sqrt_dst):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_j: Features of source nodes.\n",
    "            edge_index: Edge index tensor.\n",
    "            deg_inv_sqrt_src: Normalization factor for source nodes.\n",
    "            deg_inv_sqrt_dst: Normalization factor for target nodes.\n",
    "        \"\"\"\n",
    "        row, col = edge_index\n",
    "        norm = deg_inv_sqrt_src[row] * deg_inv_sqrt_dst[col]\n",
    "        return norm.view(-1, 1) * self.mlp(x_j)\n",
    "\n",
    "class MLP_GIN(GINConv):\n",
    "    # Just GIN with MLP applied before message aggregation \n",
    "    def __init__(self, mlp, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mlp = mlp \n",
    "    def message(self, x_j: Tensor) -> Tensor:\n",
    "        return self.mlp(x_j) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define convolution layers for each edge type\n",
    "        self.convs = torch.nn.ModuleList() \n",
    "        for i in range(config['layers']):\n",
    "            assert config['embedding'] == hidden_dim \n",
    "\n",
    "            match config['transformation']:\n",
    "                case \"relu\":\n",
    "                    node_mlp = Sequential(\n",
    "                        Linear(hidden_dim, hidden_dim),\n",
    "                        ReLU(),\n",
    "                        Linear(hidden_dim, hidden_dim)\n",
    "                    )\n",
    "                case \"linear\":\n",
    "                    node_mlp = Linear(hidden_dim, hidden_dim)\n",
    "                case \"mlp\":\n",
    "                    node_mlp = MLP(in_channels=hidden_dim, hidden_channels=hidden_dim, out_channels=hidden_dim, num_layers=3)  \n",
    "                case _:\n",
    "                    print(f\"ERROR: '{config['transformation']}' is not defined as transformation operation.\")\n",
    "                    exit(1)\n",
    "            \n",
    "            # Used for GIN-based convolution \n",
    "            gin_nn = Sequential(\n",
    "                Linear(hidden_dim, hidden_dim),\n",
    "                ReLU(),\n",
    "                Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "\n",
    "            match config['gnn']:\n",
    "                case \"SAGEConv\":\n",
    "                    relation_dict = {\n",
    "                        ('literals', 'inside', 'clauses'): SAGEConv(config['embedding'], hidden_dim, aggr='sum', project=False),\n",
    "                        ('clauses', 'contains', 'literals'): SAGEConv(config['embedding'], hidden_dim, aggr='sum', project=False),\n",
    "                        ('literals', 'negates', 'literals'): SAGEConv(config['embedding'], hidden_dim, aggr='sum', project=False),\n",
    "                    }\n",
    "                case \"SAGE_MLP\":\n",
    "                    relation_dict = {\n",
    "                        ('literals', 'inside', 'clauses'): SAGE_MLP(node_mlp, config['embedding'], hidden_dim, aggr='mean', project=False),\n",
    "                        ('clauses', 'contains', 'literals'): SAGE_MLP(node_mlp, config['embedding'], hidden_dim, aggr='mean', project=False),\n",
    "                        ('literals', 'negates', 'literals'): SAGE_MLP(node_mlp, config['embedding'], hidden_dim, aggr='mean', project=False),\n",
    "                    }\n",
    "                case \"MLPGraphNorm\":\n",
    "                    relation_dict = {\n",
    "                        ('literals', 'inside', 'clauses'): MLPGraphNorm(node_mlp, aggr='sum'),\n",
    "                        ('clauses', 'contains', 'literals'): MLPGraphNorm(node_mlp, aggr='sum'),\n",
    "                        ('literals', 'negates', 'literals'): MLPGraphNorm(node_mlp, aggr='sum'),\n",
    "                    }\n",
    "                case \"MLP_GIN\":\n",
    "                    relation_dict = {\n",
    "                        ('literals', 'inside', 'clauses'): MLP_GIN(node_mlp, nn=gin_nn, aggr='sum'),\n",
    "                        ('clauses', 'contains', 'literals'): MLP_GIN(node_mlp, nn=gin_nn, aggr='sum'),\n",
    "                        ('literals', 'negates', 'literals'): MLP_GIN(node_mlp, nn=gin_nn, aggr='sum'), \n",
    "                    }\n",
    "                case \"GINConv\":\n",
    "                    relation_dict = {\n",
    "                        ('literals', 'inside', 'clauses'): GINConv(nn=gin_nn, aggr='sum'),\n",
    "                        ('clauses', 'contains', 'literals'): GINConv(nn=gin_nn, aggr='sum'),\n",
    "                        ('literals', 'negates', 'literals'): GINConv(nn=gin_nn, aggr='sum'), \n",
    "                    }\n",
    "                case \"GATConv\":\n",
    "                    relation_dict = {\n",
    "                        ('literals', 'inside', 'clauses'): GATConv(config['embedding'], hidden_dim, add_self_loops=False), \n",
    "                        ('clauses', 'contains', 'literals'): GATConv(config['embedding'], hidden_dim, add_self_loops=False),\n",
    "                        ('literals', 'negates', 'literals'): GATConv(config['embedding'], hidden_dim, add_self_loops=False),\n",
    "                    }\n",
    "                case _:\n",
    "                    print(f\"ERROR: '{config['gnn']}' is not defined as GNN architecture.\")\n",
    "                    exit(1)\n",
    "\n",
    "            convs = HeteroConv(relation_dict, aggr='cat') # Aggregation across edge types \n",
    "            self.convs.append(convs) \n",
    "\n",
    "        self.literal_linears = torch.nn.ModuleList() \n",
    "        self.clause_linears = torch.nn.ModuleList() \n",
    "        for i in range(config['layers']):\n",
    "            match config[\"projection\"]:\n",
    "                case \"linear\":\n",
    "                    # Projection layers for each node type\n",
    "                    literals_linear = torch.nn.Linear(2*hidden_dim, hidden_dim) # Concat of clause and opposite literal embeds \n",
    "                    clauses_linear = torch.nn.Linear(hidden_dim, hidden_dim) \n",
    "                case \"mlp\":\n",
    "                    literals_linear = MLP([2*hidden_dim, hidden_dim, hidden_dim]) \n",
    "                    clauses_linear = MLP([hidden_dim, hidden_dim, hidden_dim]) \n",
    "                case \"mlp-3\":\n",
    "                    literals_linear = MLP(in_channels=2*hidden_dim, hidden_channels=hidden_dim, out_channels=hidden_dim, num_layers=3) # Concat of clause and opposite literal embeds \n",
    "                    clauses_linear = MLP(in_channels=hidden_dim, hidden_channels=hidden_dim, out_channels=hidden_dim, num_layers=3)\n",
    "                case _:\n",
    "                    print(f\"ERROR: '{config['projection']}' is not defined as projection operation.\")\n",
    "                    exit(1)\n",
    "\n",
    "            self.literal_linears.append(literals_linear) \n",
    "            self.clause_linears.append(clauses_linear) \n",
    "\n",
    "        match config[\"readout\"]:\n",
    "            case \"lstm\":\n",
    "                self.readout = LSTMAggregation(hidden_dim, hidden_dim, num_layers=3) \n",
    "            case \"mean\":\n",
    "                self.readout = global_mean_pool \n",
    "            case \"sum\":\n",
    "                self.readout = SumAggregation() \n",
    "            case \"multi\":\n",
    "                self.readout = MultiAggregation(['sum', 'mean'], mode='cat') # cat is default mode\n",
    "            case _:\n",
    "                print(f\"ERROR: '{config['readout']}' is not defined as readout operation.\")\n",
    "                exit(1)\n",
    "\n",
    "        if config['target'] == \"sat\":\n",
    "            # SAT classification \n",
    "            self.classifier = MLP(in_channels=-1, hidden_channels=hidden_dim, out_channels=1, num_layers=2) \n",
    "        else:\n",
    "            self.regressor = MLP(in_channels=-1, hidden_channels=hidden_dim, out_channels=1, num_layers=2) \n",
    "\n",
    "    def forward(self, data):\n",
    "        # HeteroConv expects node features and edge_index as a dictionary\n",
    "        x_dict = data.x_dict\n",
    "        edge_index_dict = data.edge_index_dict\n",
    "\n",
    "        # Apply heterogeneous message passing\n",
    "        jumping_literals = torch.zeros(x_dict['literals'].shape).to(x_dict['literals'].device) \n",
    "        jumping_clauses = torch.zeros(x_dict['clauses'].shape).to(x_dict['clauses'].device) \n",
    "        for conv, literal_linear, clause_linear in zip(self.convs, self.literal_linears, self.clause_linears):\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: x.relu() for key, x in x_dict.items()} \n",
    "            # Apply transformations for each node type\n",
    "            x_dict['literals'] = literal_linear(x_dict['literals'])\n",
    "            x_dict['clauses'] = clause_linear(x_dict['clauses'])\n",
    "\n",
    "            jumping_literals += x_dict['literals'] \n",
    "            jumping_clauses += x_dict['clauses'] \n",
    "\n",
    "\n",
    "        # Readout operation: Aggregate the node features of both 'literals' and 'clauses'\n",
    "        # We'll aggregate over all nodes of the graph\n",
    "        if config['jumping']:\n",
    "            x_graph = torch.cat([self.readout(jumping_literals, data.batch['literals']), self.readout(jumping_clauses, data.batch['clauses'])], 1)\n",
    "        else:\n",
    "            x_graph = torch.cat([self.readout(x_dict['literals'], data.batch['literals']), self.readout(x_dict['clauses'], data.batch['clauses'])], 1)\n",
    "        \n",
    "        if config['target'] == \"sat\":\n",
    "            if config['dropout']:\n",
    "                x_graph = F.dropout(x_graph, p=0.5, training=self.training) \n",
    "            res = torch.sigmoid(self.classifier(x_graph))\n",
    "        else:            \n",
    "            # Apply a final regressor\n",
    "            if config['dropout']:\n",
    "                x_graph = F.dropout(x_graph, p=0.5, training=self.training) # Dropout to avoid overfit\n",
    "            res = self.regressor(x_graph) \n",
    "\n",
    "        return res "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = len(dataset)*8//10 \n",
    "# Create a DataLoader for batching\n",
    "train_loader = DataLoader(dataset[:test_split], batch_size=config['batch'], shuffle=False) \n",
    "test_loader = DataLoader(dataset[test_split:], batch_size=config['batch'], shuffle=False) \n",
    "\n",
    "print(len(train_loader), len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_assignment(batch, node_type):\n",
    "    \"\"\"\n",
    "    Given a batch of graphs in a HeteroDataBatch, extract the '.ptr' and compute the 'batch' tensor that assigns each node to its graph based on the '.ptr' values.\n",
    "    \n",
    "    Args:\n",
    "        batch (HeteroDataBatch): A batch containing multiple graphs.\n",
    "    \n",
    "    Returns:\n",
    "        batch_tensor (Tensor): A tensor of size (total number of nodes across all graphs, ) containing the graph index for each node.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to collect the batch indices for each graph\n",
    "    batch_tensor = []\n",
    "    \n",
    "    # Access the 'ptr' for each graph type in the batch (should be present in the batch)\n",
    "    # Get the prefix sum (ptr) tensor\n",
    "    ptr = batch[node_type].ptr\n",
    "    \n",
    "    # Loop over each graph in the batch (using ptr values)\n",
    "    for i in range(len(ptr) - 1):\n",
    "        # Assign the same batch index for each node in the current graph\n",
    "        batch_tensor.extend([i] * (ptr[i + 1] - ptr[i]))\n",
    "    \n",
    "    # Convert the list to a tensor\n",
    "    return torch.tensor(batch_tensor, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_loader:  # Iterate in batches over the training dataset.\n",
    "        batch.batch = {'literals': compute_batch_assignment(batch, 'literals'), 'clauses': compute_batch_assignment(batch, 'clauses')} \n",
    "        batch = batch.to(DEVICE) \n",
    "        out = model(batch).squeeze()  # Perform a single forward pass. \n",
    "        loss = criterion(out, batch.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        # Perform gradient clipping by value\n",
    "        torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=5.) \n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_out = [] \n",
    "    total_y = [] \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch.batch = {'literals': compute_batch_assignment(batch, 'literals'), 'clauses': compute_batch_assignment(batch, 'clauses')} \n",
    "            batch = batch.to(DEVICE) \n",
    "            out = model(batch).squeeze() \n",
    "            total_out.append(out) \n",
    "            total_y.append(batch.y) \n",
    "\n",
    "    total_out = torch.cat(total_out, 0)\n",
    "    total_y = torch.cat(total_y, 0)\n",
    "\n",
    "    return total_out, total_y \n",
    "\n",
    "def compute_main_metric(out, y):\n",
    "    total_loss = criterion(out, y) \n",
    "    return total_loss \n",
    "\n",
    "def compute_metrics(out, y):\n",
    "    metrics = {} \n",
    "    if config['target'] == \"sat\": \n",
    "        metrics['Entropy'] = torch.nn.functional.binary_cross_entropy(out, y) \n",
    "        metrics['Accuracy'] = (torch.round(out) == y).sum() / len(out) # Count correct predictions \n",
    "    else: \n",
    "        metrics['MSE'] = torch.nn.functional.mse_loss(out, y) \n",
    "        metrics['L1'] = torch.nn.functional.l1_loss(out, y) \n",
    "        metrics['R2'] = torchmetrics.functional.r2_score(out, y) \n",
    "        metrics['MAPE'] = torchmetrics.functional.mean_absolute_percentage_error(out, y) \n",
    "        metrics['Spearman'] = torchmetrics.functional.spearman_corrcoef(out, y) \n",
    "\n",
    "    return metrics \n",
    "\n",
    "def plot_predictions(out, y, title='Test data', file=None):\n",
    "    if config['target'] == \"sat\":\n",
    "        # Confusion matrix\n",
    "        disp = ConfusionMatrixDisplay.from_predictions(y.cpu(), out.cpu() > 0.5, normalize='true', display_labels=['UNSAT', 'SAT'], cmap=plt.cm.Blues)\n",
    "        plt.title(title)\n",
    "        if file:\n",
    "            plt.savefig(file)\n",
    "        else:\n",
    "            plt.show() \n",
    "        \n",
    "    else:\n",
    "        # Regression task \n",
    "        fig, ax = plt.subplots(figsize=(5, 5)) \n",
    "        preds = out.cpu() \n",
    "        reals = y.cpu() \n",
    "\n",
    "        ax.set_xlim((0, 500.)) \n",
    "        ax.set_ylim((0, 500.))\n",
    "        ax.set_aspect(\"equal\")\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.set_ylabel(\"Predictions\")\n",
    "        ax.set_xlabel(\"Real times\")\n",
    "        ax.scatter(reals, preds, s=2, color='blue', label='Model outputs') \n",
    "        ax.axline((0, 0), slope=1., color='red', label='Perfect') \n",
    "        ax.grid()\n",
    "        ax.legend() \n",
    "        if file:\n",
    "            plt.savefig(file)\n",
    "        else:\n",
    "            plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HeteroGNN(hidden_dim=config['embedding']).to(DEVICE) \n",
    "for batch in train_loader:\n",
    "    batch.batch = {'literals': compute_batch_assignment(batch, 'literals'), 'clauses': compute_batch_assignment(batch, 'clauses')} \n",
    "    batch = batch.to(DEVICE)\n",
    "    print(summary(model, batch, max_depth=6)) \n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "random.seed(12345)\n",
    "numpy.random.seed(12345)\n",
    "\n",
    "# Initialize model \n",
    "model = HeteroGNN(hidden_dim=config['embedding']).to(DEVICE) \n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) \n",
    "if config['target'] == \"sat\":\n",
    "    criterion = torch.nn.BCELoss() # only 2 classes \n",
    "else:\n",
    "    criterion = torch.nn.MSELoss() \n",
    "\n",
    "\n",
    "# Save model configuration \n",
    "timestamp = int(time.time()) \n",
    "os.mkdir(f\"logs/{timestamp}\")\n",
    "with open(f\"logs/{timestamp}/config.txt\", 'w') as f:\n",
    "    for key in config:\n",
    "        f.write(f'{key}={config[key]}\\n') \n",
    "\n",
    "# Before training (evaluate initial random model)\n",
    "train_out, train_y = test(train_loader) \n",
    "test_out, test_y = test(test_loader) \n",
    "# Return to linear-space \n",
    "if config['log']:\n",
    "    train_out = torch.exp(train_out)\n",
    "    train_y = torch.exp(train_y)\n",
    "    test_out = torch.exp(test_out)\n",
    "    test_y = torch.exp(test_y) \n",
    "\n",
    "train_loss = compute_main_metric(train_out, train_y) \n",
    "metrics = compute_metrics(test_out, test_y) \n",
    "for metric in metrics:\n",
    "    print(f'Test {metric} = {metrics[metric]}') \n",
    "\n",
    "# Store in files \n",
    "with open(f\"logs/{timestamp}/init-metrics.txt\", 'w') as f:\n",
    "    for metric in metrics:\n",
    "        f.write(f'Test {metric} = {metrics[metric]}\\n') \n",
    "    if config['target'] == 'seconds':\n",
    "        # For regression\n",
    "        f.write(f\"{train_loss} & {metrics['MSE']} & {metrics['L1']} & {metrics['R2']} & {metrics['MAPE']} & {metrics['Spearman']}\\n\")\n",
    "plot_predictions(train_out, train_y, title='Train data', file=f'logs/{timestamp}/init-train.png') \n",
    "plot_predictions(test_out, test_y, title='Test data', file=f'logs/{timestamp}/init-test.png') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Actual training \n",
    "best_loss = 1e10 \n",
    "best_epoch = 0 \n",
    "best_model = deepcopy(model) \n",
    "history = {}\n",
    "for epoch in tqdm(range(1, config['epochs']+1), desc='Epochs'): \n",
    "    train() \n",
    "    train_out, train_y = test(train_loader) \n",
    "    train_loss = compute_main_metric(train_out, train_y) \n",
    "\n",
    "    # Save best model so far \n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss \n",
    "        best_model = deepcopy(model) \n",
    "        best_epoch = epoch \n",
    "\n",
    "        print(f'New best: {epoch:03d}, Train Loss: {train_loss:.6f}') \n",
    "\n",
    "        test_out, test_y = test(test_loader) \n",
    "        # Return to linear-space \n",
    "        if config['log']:\n",
    "            train_out = torch.exp(train_out)\n",
    "            train_y = torch.exp(train_y)\n",
    "            test_out = torch.exp(test_out)\n",
    "            test_y = torch.exp(test_y) \n",
    "\n",
    "        metrics = compute_metrics(test_out, test_y) \n",
    "        for metric in metrics:\n",
    "            print(f'Test {metric} = {metrics[metric]}') \n",
    "        \n",
    "        plot_predictions(train_out, train_y, title='Train data') \n",
    "        plot_predictions(test_out, test_y, title='Test data') \n",
    "        print() # Empty line \n",
    "\n",
    "        history[epoch] = {\n",
    "            \"train_loss\": train_loss.cpu().item()\n",
    "        }\n",
    "        history[epoch].update({ k: metrics[k].cpu().item() for k in metrics }) \n",
    "        \n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        test_out, test_y = test(test_loader) \n",
    "        metrics = compute_metrics(test_out, test_y) \n",
    "        print(f'Train Loss: {train_loss:.6f}') \n",
    "        for metric in metrics:\n",
    "            print(f'Test {metric} = {metrics[metric]}') \n",
    "        print(\"-\"*80)\n",
    "\n",
    "print(f\"Best result is {best_loss} at {best_epoch}\") \n",
    "\n",
    "\n",
    "\n",
    "# After training\n",
    "model = best_model # Evaluate the best model \n",
    "train_out, train_y = test(train_loader) \n",
    "test_out, test_y = test(test_loader) \n",
    "# Return to linear-space \n",
    "if config['log']:\n",
    "    train_out = torch.exp(train_out)\n",
    "    train_y = torch.exp(train_y)\n",
    "    test_out = torch.exp(test_out)\n",
    "    test_y = torch.exp(test_y) \n",
    "\n",
    "train_loss = compute_main_metric(train_out, train_y) \n",
    "metrics = compute_metrics(test_out, test_y) \n",
    "for metric in metrics:\n",
    "    print(f'Test {metric} = {metrics[metric]}') \n",
    "\n",
    "# Store in files \n",
    "with open(f\"logs/{timestamp}/metrics.txt\", 'w') as f:\n",
    "    f.write(f\"Train loss: {train_loss} at {best_epoch}\\n\")\n",
    "    for metric in metrics:\n",
    "        f.write(f'Test {metric} = {metrics[metric]}\\n') \n",
    "    if config['target'] == 'seconds':\n",
    "        # For regression \n",
    "        f.write(f\"{best_epoch} & {train_loss} & {metrics['MSE']} & {metrics['L1']} & {metrics['R2']} & {metrics['MAPE']} & {metrics['Spearman']}\\n\")\n",
    "plot_predictions(train_out, train_y, title='Train data', file=f'logs/{timestamp}/train.png') \n",
    "plot_predictions(test_out, test_y, title='Test data', file=f'logs/{timestamp}/test.png') \n",
    "\n",
    "with open(f\"logs/{timestamp}/architecture.txt\", 'w') as f:\n",
    "    for batch in train_loader:\n",
    "        batch.batch = {'literals': compute_batch_assignment(batch, 'literals'), 'clauses': compute_batch_assignment(batch, 'clauses')} \n",
    "        batch = batch.to(DEVICE)\n",
    "        f.write(summary(model, batch, max_depth=5)) \n",
    "        break \n",
    "# Store best model \n",
    "torch.save(best_model, f\"logs/{timestamp}/model.pt\") \n",
    "\n",
    "\n",
    "t_end = int(time.time()) \n",
    "with open(f\"logs/{timestamp}/time.txt\", 'w') as f:\n",
    "    f.write(f\"{t_end - timestamp}\") # in seconds\n",
    "\n",
    "df_history = pd.DataFrame([history[k] for k in sorted(history.keys())], index=sorted(history.keys())) \n",
    "df_history.to_csv(f\"logs/{timestamp}/history.csv\", index_label=\"Epoch\") \n",
    "\n",
    "print(f\"Best result is {best_loss} at {best_epoch}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
